{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d22b2e9",
   "metadata": {},
   "source": [
    "# Working with Delta Tables in Databricks\n",
    "Delta Lake provides reliability and performance improvements for big data workloads on Databricks.\n",
    "This notebook covers:\n",
    "- Creating Delta Tables\n",
    "- Updating and Upserting Data\n",
    "- Querying Delta Tables\n",
    "- Time Travel in Delta Lake\n",
    "- Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.appName('DeltaStudy') \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3cdb8",
   "metadata": {},
   "source": [
    "### Creating a Delta Table\n",
    "This example creates a Delta table with sample data representing customer orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacef133",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, 1, 'Laptop', 1),\n",
    "    (102, 2, 'Mouse', 2),\n",
    "    (103, 1, 'Keyboard', 1),\n",
    "    (104, 3, 'Monitor', 1),\n",
    "]\n",
    "columns = ['order_id', 'customer_id', 'product', 'quantity']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write as Delta Table\n",
    "df.write.format('delta').mode('overwrite').save('/mnt/delta/orders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d7352",
   "metadata": {},
   "source": [
    "### Upserting Data\n",
    "This code demonstrates upserts (merge operations) to add new records and update existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, '/mnt/delta/orders')\n",
    "\n",
    "new_data = [(105, 2, 'Laptop', 1), (102, 2, 'Mouse', 3)]\n",
    "columns = ['order_id', 'customer_id', 'product', 'quantity']\n",
    "new_df = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "delta_table.alias('old').merge(\n",
    "    new_df.alias('new'),\n",
    "    'old.order_id = new.order_id'\n",
    ").whenMatchedUpdate(set={'quantity': 'new.quantity'})\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute()\n",
    "\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75469ef7",
   "metadata": {},
   "source": [
    "### Querying Delta Tables\n",
    "Delta tables can be queried like any Spark DataFrame. Hereâ€™s an example retrieving all records from the orders table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load('/mnt/delta/orders')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53133d71",
   "metadata": {},
   "source": [
    "### Time Travel with Delta Tables\n",
    "Delta Lake supports time travel, which allows you to query data as it existed at a previous version.\n",
    "Below is an example querying an older version of the orders table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4228527",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v0 = spark.read.format('delta').option('versionAsOf', 0).load('/mnt/delta/orders')\n",
    "df_v0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26c44e",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "- **Partition Delta Tables**: Partitioning helps improve performance for large tables.\n",
    "- **Vacuum Unused Data**: Run the `VACUUM` command to remove old data files that are no longer needed.\n",
    "- **Optimize with ZORDER**: Use ZORDER by frequently filtered columns to improve query speed.\n",
    "\n",
    "These practices help maintain efficiency and reduce storage in Delta Lake environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597077c5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we explored Delta tables and covered:\n",
    "- Creating and writing to Delta tables\n",
    "- Upserting records in Delta tables\n",
    "- Querying and using time travel\n",
    "- Best practices for Delta Lake\n",
    "\n",
    "Delta tables provide an efficient and powerful data management solution for big data environments, especially with Databricks."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
